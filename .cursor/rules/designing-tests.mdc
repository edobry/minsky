---
description: Guidelines for writing effective, maintainable tests with proper isolation, data management, and thorough coverage
globs: ["**/*.test.ts", "**/*.spec.ts", "**/test/**/*"]
alwaysApply: false
---
# Designing Tests

Write comprehensive, maintainable tests following these principles:

## Test Structure & Organization
- Use `describe`/`it` blocks to organize tests in a clear hierarchy
- Name tests with clear, action-oriented descriptions of what's being tested
- Follow the Arrange-Act-Assert pattern for test clarity
- Keep test files alongside the code they test with matching names (e.g., `file.ts` and `file.test.ts`)

## Environment Isolation
- Create temporary test directories for filesystem operations
- Use `beforeEach`/`afterEach` hooks for consistent setup and cleanup
- Reset test state between tests to prevent cross-test contamination
- Explicitly clean up resources even when tests fail (using `afterEach`)
- Never depend on global state or other tests' side effects

## Test Data Management
- Create test fixtures with meaningful, predictable test data
- Use helper functions to set up test state
- Maintain clear separation between test setup and assertions
- Make test data representative of real usage but simple enough to reason about
- Don't use test data that could produce flaky tests (e.g., current date/time)

## Mocking & Stubbing
- Mock external services and dependencies for deterministic tests
- Use the simplest mocking approach that meets your needs
- Prefer explicit mocks over automatic/magic mocking
- Verify mock calls when testing integration points
- Reset mocks between tests

## Error & Edge Case Testing
- Test both success and failure paths explicitly
- Include tests for edge cases and boundary conditions
- Test handling of empty inputs, null values, and invalid data
- Verify error objects, messages, and types
- Test graceful handling of resource failures (network, filesystem, etc.)

## Assertion Best Practices
- Use specific, precise assertions (e.g., `toContain` vs `toBeTruthy`)
- Test only what matters â€“ avoid over-specifying implementation details
- Verify side effects (e.g., file creation/deletion) in addition to function returns
- Keep assertions focused on a single behavior per test
- For complex objects, assert only on relevant properties

## Setup & Teardown
- Use the minimum setup necessary for each test
- Prefer local setup within tests over complex shared fixtures
- Ensure proper cleanup to avoid test pollution
- Make tests resilient to different execution environments
- Document environment requirements in test files

## Coverage Guidelines
- Aim for high coverage of business logic and error handling
- Don't obsess over 100% coverage at the expense of test quality
- Focus on testing behaviors rather than implementation details
- Include tests for both API and CLI/UI interfaces
- Test different output formats (text, JSON, etc.) for data-producing functions

## Guidelines for Writing Effective, Maintainable Tests

### 1. Focus on Core Functionality
- Write tests that verify the essential behavior and outcomes of a module or function, not its internal implementation details.
- Prefer black-box testing: test the public API and observable effects, not private state or internal calls.

### 2. Avoid Complex Mocking
- Do not use complex or brittle mocking patterns, especially those that require deep knowledge of module internals or rely on patching module properties.
- If mocking is necessary, use simple, explicit stubs or dependency injection.
- Avoid mocking file system operations, process environment, or global state unless absolutely required.

### 3. Use Dependency Injection
- Structure code so that dependencies (e.g., services, database, file system) can be injected for testing.
- In tests, provide minimal mock implementations for dependencies that return predictable results.
- This enables tests to be simple, reliable, and decoupled from implementation details.

### 4. Avoid File System Operations in Tests
- Do not create, modify, or delete files or directories in tests unless the test is specifically for file I/O.
- Prefer in-memory or stubbed data for testing logic.
- If file system operations are unavoidable, use temporary directories and ensure cleanup is robust.

### 5. Prefer Simplicity and Maintainability
- Write tests that are easy to read, understand, and maintain.
- Avoid over-specifying behavior that is likely to change as implementation evolves.
- Use clear, descriptive test names that specify the intended behavior.

### 6. Test Only What Matters
- Do not test implementation details that are not part of the module's contract.
- Avoid asserting on the number of function calls, internal state, or specific log output unless it is part of the public API.

### 7. Use Proper Test Structure
- Group related tests using `describe` blocks.
- Use `test` or `it` for individual test cases.
- Use setup/teardown hooks (`beforeEach`, `afterEach`) only when necessary.

### 8. Document Test Rationale
- When a test uses a non-obvious pattern (e.g., a stub, a workaround for a test runner limitation), document why.

### 9. Prefer Integration Over End-to-End for CLI
- For CLI commands, prefer integration tests that invoke the command handler directly with injected dependencies, rather than spawning processes or writing/reading files.
- Only use end-to-end CLI tests for critical user flows that cannot be covered by integration tests.

### 10. Update Tests When Refactoring
- When refactoring code, update tests to match the new structure, but do not overfit tests to the implementation.
- Remove or rewrite tests that are no longer relevant or that test implementation details.

See also: `.cursor/rules/testing-boundaries.mdc` for specific guidance on CLI and framework testing boundaries.

---

_This rule was updated after task#022 to reflect the insight that maintainable tests should focus on core functionality, use dependency injection, avoid complex mocking, and minimize reliance on file system operations or process state. See the completion log for task#022 for details._
