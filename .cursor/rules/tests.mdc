---
description:
globs:
alwaysApply: true
---
Rule Name: tests
Description:
Test Coverage and Quality Requirements:

1. **When to Run Tests**
   - Run tests after ANY change to:
     - Source code files (*.ts, *.js)
     - Test files (*.test.ts, *.spec.ts)
     - Configuration files that affect test behavior
   - Do NOT run tests for:
     - Documentation changes (*.md)
     - Comment-only changes
     - Formatting-only changes (unless they affect test output)

2. **Which Tests to Run**
   - Run all tests in the affected package/module
   - For changes to shared utilities or core functionality, run all tests
   - Use `bun test` for the default test suite
   - Use `bun test --coverage` when making significant changes

3. **Test Success Criteria**
   - All tests must pass (no failures)
   - No new test warnings should be introduced
   - Test coverage should not decrease for modified files
   - Flaky tests should be fixed or marked as such

4. **Test Development**
   - When writing new tests:
     - Only the new/modified test files should be failing
     - Document why tests are failing in the commit message
     - Fix failing tests before completing the change
   - When modifying existing tests:
     - Ensure changes maintain or improve test coverage
     - Update test documentation if behavior changes
     - Consider adding new tests for edge cases

5. **Test Environment**
   - Run tests in the development environment
   - Ensure all required dependencies are installed
   - Document any environment-specific requirements

6. **Test Reporting**
   - Document any skipped tests and why
   - Note any changes to test coverage
   - Flag any new test warnings or flaky tests

7. **Test Integrity Requirements**
   - **ABSOLUTELY FORBIDDEN:**
     - Creating placeholder/mock tests to bypass failures
     - Creating tests that always pass without testing actual functionality (e.g., `expect(true).toBe(true)`)
     - Commenting out failing tests instead of fixing them
     - Using `test.skip()` or similar to avoid test failures
     - Any practice that hides or ignores test failures
   - All test failures MUST be properly addressed by either:
     - Fixing the underlying code issue
     - Updating test expectations with proper justification
     - Refactoring the test to properly test the intended behavior
   - If a test is failing:
     - Document the failure in detail
     - Create a plan to fix it
     - Get approval for the fix approach
     - Implement the fix
     - Verify the fix with proper test coverage
   - Never merge code with:
     - Failing tests
     - Placeholder tests
     - Skipped tests without documented justification
     - Mock tests that don't actually test functionality

8. **ZERO TOLERANCE FOR FAKE TESTS**
   - There is NEVER an acceptable reason to create fake/mock tests that automatically pass
   - Tests that use `expect(true).toBe(true)` or similar constructs that don't test actual behavior are strictly prohibited
   - When encountering test failures:
     1. First attempt to fix the test properly to test the actual behavior
     2. If that's not immediately possible, leave the test failing and document why
     3. If a temporary solution is needed while investigating, use `test.todo()` with detailed explanations
   - Any PR or commit containing fake tests will be automatically rejected
   - This is a critical part of maintaining code quality and is non-negotiable

9. **Enforcement Mechanism**
   - All test files must be periodically scanned for patterns indicating fake tests
   - Code review must specifically check for fake/bypassed tests
   - Automated linting should detect and flag suspicious test patterns
   - Before submitting changes, verify all tests are properly testing functionality

This is a zero-tolerance policy. Any violation will require immediate remediation.

# Tests

## Best Practices Cross-Reference
- See also: dont-ignore-errors, designing-tests, minsky-workflow, rule-map.mdc
- This rule governs: batch verification, verification checkpoints, and test pass/fail gating.

## Requirements (Revised)
- You MUST run all relevant tests after any change to source, test, or config files.
- You MUST NOT proceed with further changes until all tests pass and all errors are fixed.
- You MUST use batch verification (lint, type check, test) after each set of related changes.
- You MUST reference this rule when verifying code before merging or marking a task complete.
